2020-07-19 07:51am ET

Primary vs. derived.
Remembering why I turned off Thunderbird "Gloda" (Global Search Database)
https://support.mozilla.org/en-US/kb/global-search

Because it was a multi-gigabyte index file that kept changing
and which Mac TimeMachine kept backing up each time....
Causing long delays and bloating backups and chewing up backup drive disk space.

I am often thinking (and constrained) by concern of how to make Twirlip/Pointrel archives
easy to backup incrementally. Challenging as hard to have changing indexes then...
Of course, you can selectively exclude files from backups.
And maybe that is the answer?
To exclude backing up big files that are derived -- tradeoff of storage vs. time.

In rough calculations, on Lenovo flex14 laptop, 
Thunderbird is indexing about 50 messages per second.
Thunderbird is adding (initially) about 10K (20K?) per message to global-messages-db.sqlite.
It seems it will take about five hours to index a million messages.
It will end up with a ~10GB index file.
The "Activity Manager" explains what is going on.
It says how many emails have been indexed of how many total and how long it took.

So, is that acceptable? Interesting how people must think it is.

[In the end, the resulting index file is just under 4.1 GB.
It took about 8.5 hours including indexing RSS feeds of 100ks of tweets.
For some reason one rss feed at the end took 30 minutes to index ~100K messages
-- maybe the laptop was sleeping?
It was unfortunate I could not copy and paste from the Activity Manager window.]

Laptop getting warm as it runs the indexing process... 
Thunderbird running at 7% CPU load.
~20% load overall. Maybe one full vcpu thread of eight?
How much heat from flash drive itself?

Later, about 110K messages indexed so far in about 33 minutes and about 450MB.
So, about 55 messages a second, and about 4K per message of indexing.
Maybe my Sent messages where is started are longer than average. :-)

Ideally Linux would have some way of labelling derived files 
or putting them in a special folder (under /var?) that is not backed up.

Until then: https://linuxize.com/post/how-to-exclude-files-and-directories-with-rsync/
$ rsync -a --exclude 'file.txt' src_directory/ dst_directory/
$ rsync -a --exclude '*.jpg*' src_directory/ dst_directory/
$ rsync -a --exclude={'file1.txt','dir1/*','dir2'} src_directory/ dst_directory/
$ rsync -a --exclude-from='exclude-file.txt' src_directory/ dst_directory/
Where exclude-file.txt is:
file1.txt
dir1/*
dir2

Maybe I need to accept there will be a large index file somewhere which is derived
and which optionally will not be backed up?
Although five house (in this case) is a long time to wait until something is operational again.
What if it was ten million messages or a billion messages?

----

Have been thinking recently about storage similar to TimeMachine and such like so:
Directories for each time period (week, day, hour, minute, whatever).
Subdirectories reflecting a hierarchy of "streams" (which can be whatever -- discussions, text files, images, etc.)
A stream can have substreams (/somewhere/mystream/mysubstream-a,b,c).
Each stream is one file with incremental transactions.
A stream could also be a directory with one transaction per file, but too much overhead of file system?
To find everything in a stream, you move across multiple time top-level directories,
and you collect all the transactions related to that stream.

Where is the index for a stream (or hierarchy of streams?).
If could be in a separate index directory parallelling all the streams.
But if you have a global stream directory with everything -- so you need oen per stream?
So could maybe just have one big index file?

That index file is perhaps not backed up as it is derived...
But is would take a long time to rebuild.
Would be nice if could backup index file incrementally too.
The CouchDB approach to a growing index with the end referring to immutable beginning?
But done as segments of the index in different files?
Maybe work in progress index segment and then permanent segments?
Nice idea but too much for now? Distraction from UI and apps.

---

So just use one big SQLite file?
And have SQL API to use it by the client to the server?

Alternative is a triplestore index approach (with or without SQLite) with a triples API.

Or could use CouchDB and have a document store of transactions...

SQL API to sqlite files is more general?
Then can open any SQLite file on the server.

---

The reason for some index is to support Twirlip apps making sense of data on rest of disk.

---

Looking at sqlite wrappers, it seems like sqlite has about 80 API functions that need to be wrapped.
http://spiderape.sourceforge.net/plugins/sqlite/

That gives me pause. Maybe easier to just use Pointrel API with a few API functions
like addTriple (or addTransaction) and findTriple? And maybe a few others.
ALso sqlite requires (for efficiency) a connection and a cursor.
Maybe no way around cursor? Unless use original Pointrel approach of linked list of same field?
Then a cursor is just a pointer to the file position of the last triple returned.

2022-06-21 9:27pm ET

Comparing ideas for raw storage of triples -- simplicity vs. expandability (where only last field can have spaces -- or maybe unescaped spaces):

    {"a":"collageNode:1270111357464769636","b":"modificationDate","c":"date:2013-01-06T09:33:20.986Z"}

    collageNode:1270111357464769636 modificationDate date:2013-01-06T09:33:20.986Z

    replace collageNode:1270111357464769636 modificationDate date:2013-01-06T09:33:20.986Z

2022-06-22 8:30am ET

Or can include a version field at start for upgrades (including json):

    v1 replace collageNode:1270111357464769636 modificationDate date:2013-01-06T09:33:20.986Z

    v2 2013-01-06T09:33:20.986Z al.selvin@example.com replace collageNode:1270111357464769636 modificationDate date:2013-01-06T09:33:20.986Z

Thinking on how I like the content-type in the last field. 
And in the first for the class of a uuid.

But I don't like how application code gets littered with error-prone string manipulation like:
    t.addTripleABC(uuid, "label", "text:" + newLabel)

Might be more explicit to have:

    t.addTripleABTC(uuid, "label", "text", newLabel)

Or:

    t.addTripleABCT(uuid, "label", newLabel, "text")

Where the last field defaults to "text" if not specified.

And maybe then find(a, b, c) functions with and without type for return result or with auto-conversion?

I'm OK with the first UUID field being always of that form,
although it would be good to hav a uuid function like:

    const uuid = makeUUID("collageNode")

instead of:

    const uuid = "collageNode:" + UUID.uuidv4()
    
Although one could argue that a type is superfluous if have a type implicit in the B field:

    collageNode:1270111357464769636 modificationDate date:2013-01-06T09:33:20.986Z

Versus:

    collageNode:1270111357464769636 modificationDate 2013-01-06T09:33:20.986Z

But I do like the consistency of every C field having a type, even if just a handful
of text, json, number, date, or some class. 

I handled that partially before by automatically converting to and from JSON.

But JSON does not have dates.
But they could be strings the application converts -- or even leaves as strings.

(Idea of having one sentence per line in electronic documents?
No extra cost for files -- but more scrolling.
But maybe easier to read conceptually?)

It feels a little odd to not have a type for the B field, even as it is simpler and maybe not needed.
I see the argument for XML-like namespacing though.
But you can either add or nto add a type or namespace yourself:

    collageNode:1270111357464769636 myApp:modificationDate date:2013-01-06T09:33:20.986Z

Or even:

    v1 myApp/collageNode:1270111357464769636 myApp/field:modificationDate myApp/date:2013-01-06T09:33:20.986Z

One risk is that once you start using text fields of some sort you get stuck --
in the sense that it is not easy to change those names in the stored files.
In a database, you could do some sort fo system-wide upgrade perhaps.
Fundamental limitation of these sorts of text fields.

I know in past though that I did not like have six required fields to add things:

    addTripleTATBTC("myApp/collageNode", "1270111357464769636", "myApp/field", "modificationDate", "myApp/date", "2013-01-06T09:33:20.986Z")

Realizing having six things exceeds easy mental capacity of three or four things in memory.

From: https://en.wikipedia.org/wiki/XML_namespace
> Attributes are never subject to the default namespace.
> An attribute without an explicit namespace prefix is considered not to be in any namespace.

XML Elements may be in a default namespace though after defining one.

What of not using a colon?

    v3 myApp/collageNode/1270111357464769636 myApp/field/modificationDate myApp/date/2013-01-06T09:33:20.986Z

Or pipe?

    v3 myApp|collageNode|1270111357464769636 myApp|field|modificationDate myApp|date|2013-01-06T09:33:20.986Z

Where pipes would have to be escaped if in text?
    
    v3 myApp|collageNode|1270111357464769636 myApp|field|detail myApp|text|Some text with a pipe next \| that needs to be escaped.

Although the leaning-forward slash (/) is so common as a separator for levels.
Is it more standard to use it or is it confusing where will get munged by other code?
And where slash will need to be escaped in every path stored in a text field?

The colon is also potentially conflicting with JSON fields or URLs.

The colon for type also conflicts conceptually with JSON where the colon is used with a field name not a type.

Related: https://stackoverflow.com/questions/492090/least-used-delimiter-character-in-normal-text-ascii-128

Could use unit separator instead of space?

  https://unicode-table.com/en/001F/

Let's see what a "unit separator" looks like here: 

A small empty box. Seems confusing...

And hard to type for manual editing.

Let's see:

    v4 myApp|collageNode|1270111357464769636myApp|field|detailmyApp|text|Some text with a pipe next \| that needs to be escaped.

Not impossible though.

But then would be issue if try to use other separators and they are also boxes.

Could also do:

    v5 myAppcollageNode1270111357464769636 myAppfielddetail myApptextSome text with a pipe next \| that needs to be escaped.

Again, maybe confusing and hard to type or read.

I kind of like the forward slash the best as a "path".
But I worry it will be harder to read escaped:

    v6 myApp/collageNode/1270111357464769636 myApp/field/URL myApp/URL/https:\/\/simon.buckinghamshum.net\/2015\/11\/al-selvin-memories-tributes\/

I guess it is not that bad...

But contrast with pipes:

    v6 myApp|collageNode|1270111357464769636 myApp|field|URL myApp|URL|https://simon.buckinghamshum.net/2015/11/al-selvin-memories-tributes/

For the common case.

-----

Thinking that of use the pipe then can use UUIDs as file names if do object storage.
I liked that idea in Twirlip7 even if it was not clear what the pool of objects would be.
But "myApp|collageNode|1270111357464769636" can be a file name by itself.
In Twirlip7 did the sha256 hash of those instead though.

On the other hand, with slashes, could just make subdirectories...

But get a sense this is all fun and interesting to think about but also kind of arbitrary.
Whatever way I choose, I will need to escape some characters -- at the least newlines
if want one triple per line.
Otherwise would need an end character or symbol which would need to be escaped
or chosen everytime so does nor appear in the text.
I did that before too.

Right now thigns are in JSON, so the question is, what is the benefit of switching?
Less computer cycles to parse?
Easier for humans to read?
More expandable with (escaped) pipes to have hierarchy of namespaces (or none at all)?
Maybe harder to parse with escaped pipes? Harder to break into chunks?
Or just limit number of splits?

Not sure how to split on pipes if some are escaped and no split on escaped pipes?

Ideas on splitting:
https://stackoverflow.com/questions/34045019/split-string-by-comma-but-not-escaped-in-javascript

But anything coded by hand unlikely to be as fast as built-in JSON parsing.

====

2022-07-21

Ways to name files when you have a lot of things
and where manually naming a file for each item is impractical or undesired:

* Hash of content
* Sequence numbers as add each file
* Timestamp when adding file
* UUID from content (but may not be unique)
* Random number
* A mix of the above

Assuming probably using with nested directories with parts of name.

Can also add some extra sharding unique info 
like author or archive uuid or machine/process uuid.

Examples: emails, chat messages, downloaded web pages or parsed parts, JSON objects,
pictures, screenshots, recordings, imported files for an archiver, file versions.

====

2022-07-23

Most important is idea of shared standards, not implementation (which can be replaced).
Emphasizing standards -- such as they were in 1991 -- is what the web got right.

Dojo notion of a "Store" from original Dojo and dgrid.
Reflected in idea of "Notebook" from Twirlip7.

----

Idea of successive refinement of data through multiple versions 
and additions and deletions and connections.
Supporting organic growth of data but also pruning and reshaping.
And also "time travel" to revisit past data organization.
When the past is secure and revisitable, can move forward with confidence.
Idea of "undo" or "history" so people can explore and create with confidence.

====

2022-07-24

Thinking on Indexing... Interwoven with storage.
Pointrel hit a sweet spot for early personal computers (in my opinion)
where you could find stuff with a little effort
but the implementation was relatively simple
and there was a reasonable tradeoff in storage cost versus lookup time
for many common applications.
Essentially, you could quickly find all triples that had the same A, B, or C field.
And those triples would tend to define an "object" for common A fields,
a use of a type or value for C fields,
and objects that had a certain type of field for C fields.
Not perfect, but a good tradeoff?
That was done by maintaining linked lists of triples with the same A, B, or C field.

With Twirlip7, I realized that -- using the file system too lookup a hash of a value --
that you could maintain files that had all the triples using the same A, B, or C relations.
I was trying to filter out some of these by just doing "interesting" relations.

But the most critical was probably the A field.
Files that had all the triples with the same A field essentially defined an object.
It was a way of easily loading all the data for an object from one file.

Maybe doing B and C fields are less interesting? B is maybe least?
C is maybe of use when you want to look up all objects of some type.
Alternative there is to have a "class" object that tracks all instances somehow.

Trying to decide whether to take that approach with implementing a message
storing system -- like if parse mbox files into individual emails.

Maybe give up on B and C lists as too "noisy" for the value,
and as with Smalltalk, maintain objects explicitly that track things you want
to lookup again later?

----

Not sure if still need to store triples in spaces if store them in object files?

Using the file system in this way to store one object per file (and also maybe
B and C files) is somewhat like maildir and storing one email per file.
More general as any object not just emails...

But what about indexing?

You need to maintain index objects. Unless go with B and C files and use
them as indexes in some way (like adding a triple for each use of a word).

----

Limitations of previous Pointrel triplestore approaches:

If you change a C field, you will still find the previous C-value triple.
So, you would always need to check if it has been deleted, which is more work.
That confusion could be reduced a little perhaps by having insert/ add/replace/deleted
messages used when adding a triple -- to tell whether it replaces another triple
or if it is used alongside other triples as a list. Or whether it deletes another triple.
But still need to so extra lookup work across the B field to see if alternative?
Only an issue when looking up by C? Looking up by B would give you the latest C.
Assuming essentially that A field have object permanence or delete marker.

General issue where you might sometimes want to have only one value for a B field
and others where multiple values or B are OK. Example: has-height vs has-child.
Also, if you support multiple values like has-child how do you delete one?
Could store an array, but that breaks model of C field being just on item.
Can do "has-parent" from child, but that requires B-C or C-B index --
and also if you change the parent then you have previous issue with false positive
on old triple that was overridden.

Can't find things by range. If you want something that has a timestamp between
X and Y dates, you would need to search everything or alternatively search on every
possible timestamp. Or maybe (thinking now) use a b-tree or oct-tree like approach
where hae previously added relationships for larger ranges of time and you narrow those down.
Thought about this a bunch when seeing what Apache Accumulo could do.

Issue when want to find a match with two fields, like "has-type email" and "from X".
The system can qive you a list of one or the other but you then need to filter them further
yourself. Or you need to look for the overlap of two sets of results.
Previously had thought of tracking number of matches for triples so could pick
the one of two or more with the least matches to get fewest results to further filter.

Deleting stuff in general becomes an ad-hoc implementation specific thing usually --
where the data is not deleted, just ignored.

Might be more issues too. But concisely, the Pointrel system has had issues with:
* false positive matches of old data
* supporting one-to-many relationships
* finding matches by ranges
* finding matches involving multiple fields
* deleting stuff

Can see the benefits of a regular SQL database for good indexing.
And also benefits of storing and updating JSON objects over triples for coherent objects.

Time to abandon triples finally for PostgreSQL? Or some way triples are good enough?
But issue with RDBMS where can have only one common database layout,
which makes it harder for applications to be independent modules.

Possibilities: PostegreSQL, SQLite, Accumulo, Craigslist's clblob engine.
Others like IndexedDB, CouchDB, Neo4j, etc?  https://db-engines.com/en/ranking

But I really like the idea of data stored in the filesystem in wway it can
be easily and efficiently incrementally backed up by TimeMachine or rsync...

But adding so many triples for matches for text would create some huge files.
Maybe put in place sharding/splitting or such from the start?
So that have ObjectID.1 file which when it gets too big 
then a ObjectID.2 file is created and so on?
And the Twirlip code knows to return results from them all for an object?
